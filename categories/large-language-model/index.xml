<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Large Language Model on zhoukuncheng's Personal Blog</title><link>https://zhoukuncheng.github.io/categories/large-language-model/</link><description>Recent content in Large Language Model on zhoukuncheng's Personal Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 05 May 2024 16:11:34 +0800</lastBuildDate><atom:link href="https://zhoukuncheng.github.io/categories/large-language-model/index.xml" rel="self" type="application/rss+xml"/><item><title>LLM Learning Series 2. Function Calling</title><link>https://zhoukuncheng.github.io/posts/llm-2-function-calling/</link><pubDate>Sun, 05 May 2024 16:11:34 +0800</pubDate><guid>https://zhoukuncheng.github.io/posts/llm-2-function-calling/</guid><description>Introduction For typical LLM interactions, a single prompt or a few rounds of chat are sufficient to achieve the desired result. However, some tasks require the LLM to access information beyond its internal knowledge base. For example, retrieving today&amp;rsquo;s weather information for a specific city or searching for a particular anime necessitates calling external functions. What is Function Calling? Function calling in LLMs empowers the models to generate JSON objects that trigger external functions within your code. This capability enables LLMs to connect with external tools and APIs, expanding their ability to perform diverse tasks. Function Calling Execution Steps User calls LLM API with tools and a user prompt: The user provides a prompt and specifies the available tools. What is the weather like in San Francisco? Define Tool Schema tools = [ { &amp;#34;type&amp;#34;: &amp;#34;function&amp;#34;, &amp;#34;function&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;get_current_weather&amp;#34;, &amp;#34;description&amp;#34;: &amp;#34;Get the current weather in a given location&amp;#34;, &amp;#34;parameters&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;object&amp;#34;, &amp;#34;properties&amp;#34;: { &amp;#34;location&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34;, &amp;#34;description&amp;#34;: &amp;#34;The city and state, e.g. San Francisco, CA&amp;#34;, }, &amp;#34;unit&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34;, &amp;#34;enum&amp;#34;: [&amp;#34;celsius&amp;#34;, &amp;#34;fahrenheit&amp;#34;]}, }, &amp;#34;required&amp;#34;: [&amp;#34;location&amp;#34;], }, }, } ] Define Dummy Function # Example function hard coded to return the same weather def get_current_weather(location, unit=&amp;#34;fahrenheit&amp;#34;): &amp;#34;&amp;#34;&amp;#34;Get</description></item><item><title>LLM Learning Series 1. Prompt Engineering</title><link>https://zhoukuncheng.github.io/posts/llm-1-prompt-engineering/</link><pubDate>Sat, 27 Apr 2024 11:11:34 +0800</pubDate><guid>https://zhoukuncheng.github.io/posts/llm-1-prompt-engineering/</guid><description>Mastering the Art of LLM Prompts Large Language Models (LLMs) like GPT-4 and Claude possess remarkable capabilities. However, unlocking their full potential requires effective communication through well-crafted prompts. This guide delves into the art of prompt engineering, offering a step-by-step approach â€“ from fundamental principles to advanced techniques â€“ to harness the true power of LLMs.
Step 1: Choosing the Optimal Model Latest and Greatest: Newer models like GPT-4 Turbo offer significant advantages over predecessors like GPT-3.5 Turbo, including smoother natural language understanding. For simpler tasks, extensive prompt engineering may be less crucial.
Benchmarking: Utilize resources like LLM leaderboards and benchmark results to compare models and identify the best fit for your specific needs.
Examples:
For nuanced language translation, GPT-4 Turbo&amp;rsquo;s contextual understanding is likely superior to older models. For tasks that require both capabilities and speed, the Llama-3-70b open-source model is an excellent option. Step 2: Establishing Clear Communication Clarity and Specificity Explicit Instructions: Treat the LLM as a collaborator requiring clear direction. Define the task, desired outcome, format, style, and output length explicitly, avoiding ambiguity.
Contextual Grounding: Provide relevant background information and context to guide the LLM towards the desired response, considering the intended audience and purpose.
Separation of Concerns: Clearly separate instructions from context using ### or &amp;quot;&amp;quot;&amp;quot;.</description></item><item><title>å€ŸåŠ© LLM å’Œ Telegram æœºå™¨äººï¼Œè®©èƒŒå•è¯ä¸å†æ¯ç‡¥</title><link>https://zhoukuncheng.github.io/posts/llm-vocabulary-reminder/</link><pubDate>Fri, 19 Apr 2024 00:11:00 +0800</pubDate><guid>https://zhoukuncheng.github.io/posts/llm-vocabulary-reminder/</guid><description>èƒŒè‹±è¯­å•è¯æ€»æ˜¯ Abandonï¼Ÿ å‘å¤šé‚»å›½ ğŸ¦‰ å–ç»ï¼Œè®©å•è¯ä¸»åŠ¨æé†’è‡ªå·± èƒŒå•è¯ï¼Œåœ¨è‹±è¯­å­¦ä¹ ä¸­å®åœ¨æ— æ³•é¿å…ï¼Œä»å°å­¦åˆ°ç ”ç©¶ç”Ÿï¼Œç”šè‡³éƒ¨åˆ†å·¥ä½œå²—ä½ä¹Ÿéœ€è¦è®°å•è¯ã€‚ ä½†æŠ±ç€å•è¯ä¹¦å•ƒï¼Œæˆ–è€…æ‰‹æœºä¸Šä¸€æ¿ä¸€çœ¼èƒŒå•è¯ï¼Œæ•ˆç‡å®åœ¨å¤ªä½ã€‚ç°åœ¨ LLM è¿™ä¹ˆç«ï¼Œä¸ºä½•ä¸åˆ©ç”¨èµ·æ¥ï¼Ÿ æ¯•ç«Ÿ LLM ä¸­é—´çš„ L å°±æ˜¯ Language çš„æ„æ€ï¼ŒLLM å¯¹ä»˜å…¶ä»–ä¸¥è°¨ä»»åŠ¡å¯èƒ½å·®ç‚¹æ„æ€ï¼Œä½†è‹±è¯­ç­‰è¯­è¨€å¯æ˜¯å®ƒçš„å¼ºé¡¹ï¼Œèƒ½ç”¨æŠ€æœ¯è§£å†³é—®é¢˜ï¼Œå°±ä¸è¦éº»çƒ¦è‡ªå·±ï¼ æœ¬æ–‡å°±æ¥åˆ†äº«ä¸€ä¸‹ï¼Œæ€ä¹ˆç”¨æ¬§è·¯è¯å…¸ APIã€LLM</description></item></channel></rss>