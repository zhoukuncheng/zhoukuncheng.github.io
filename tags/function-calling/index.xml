<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Function Calling on zhoukuncheng's Personal Blog</title><link>https://zhoukuncheng.github.io/tags/function-calling/</link><description>Recent content in Function Calling on zhoukuncheng's Personal Blog</description><generator>Hugo -- 0.153.3</generator><language>en</language><lastBuildDate>Sun, 05 May 2024 16:11:34 +0800</lastBuildDate><atom:link href="https://zhoukuncheng.github.io/tags/function-calling/index.xml" rel="self" type="application/rss+xml"/><item><title>LLM Learning Series 2. Function Calling</title><link>https://zhoukuncheng.github.io/posts/llm-2-function-calling/</link><pubDate>Sun, 05 May 2024 16:11:34 +0800</pubDate><guid>https://zhoukuncheng.github.io/posts/llm-2-function-calling/</guid><description>&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;For typical LLM interactions, a single prompt or a few rounds of chat are sufficient to achieve the desired result. However, some tasks require the LLM to access information beyond its internal knowledge base. For example, retrieving today&amp;rsquo;s weather information for a specific city or searching for a particular anime necessitates calling external functions.&lt;/p&gt;
&lt;h3 id="what-is-function-calling"&gt;What is Function Calling?&lt;/h3&gt;
&lt;p&gt;Function calling in LLMs empowers the models to generate JSON objects that trigger external functions within your code. This capability enables LLMs to connect with external tools and APIs, expanding their ability to perform diverse tasks.&lt;/p&gt;</description></item></channel></rss>